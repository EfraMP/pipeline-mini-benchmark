@article{Shuken2023,
   abstract = {Mass spectrometry is unmatched in its versatility for studying practically any aspect of the proteome. Because the foundations of mass spectrometry-based proteomics are complex and span multiple scientific fields, proteomics can be perceived as having a high barrier to entry. This tutorial is intended to be an accessible illustrated guide to the technical details of a relatively simple quantitative proteomic experiment. An attempt is made to explain the relevant concepts to those with limited knowledge of mass spectrometry and a basic understanding of proteins. An experimental overview is provided, from the beginning of sample preparation to the analysis of protein group quantities, with explanations of how the data are acquired, processed, and analyzed. A selection of advanced topics is briefly surveyed and works for further reading are cited. To conclude, a brief discussion of the future of proteomics is given, considering next-generation protein sequencing technologies that may complement mass spectrometry to create a fruitful future for proteomics.},
   author = {Steven R. Shuken},
   doi = {10.1021/acs.jproteome.2c00838},
   issn = {15353907},
   issue = {7},
   journal = {Journal of Proteome Research},
   title = {An Introduction to Mass Spectrometry-Based Proteomics},
   volume = {22},
   year = {2023}
}
@article{Sinha2020,
   abstract = {Mass spectrometry (MS)-based proteomics is the most comprehensive approach for the quantitative profiling of proteins, their interactions and modifications. It is a challenging topic as a firm grasp requires expertise in biochemistry for sample preparation, analytical chemistry for instrumentation and computational biology for data analysis. In this short guide, we highlight the various components of a mass spectrometer, the sample preparation process for conversion of proteins into peptides, and quantification and analysis strategies. The advancing technology of MS-based proteomics now opens up opportunities in clinical applications and single-cell analysis.},
   author = {Ankit Sinha and Matthias Mann},
   doi = {10.1042/bio20200057},
   issn = {17401194},
   issue = {5},
   journal = {Biochemist},
   title = {A beginner's guide to mass spectrometry—based proteomics},
   volume = {42},
   year = {2020}
}
@article{Stewart2023,
   abstract = {The growing trend toward high-throughput proteomics demands rapid liquid chromatography-mass spectrometry (LC-MS) cycles that limit the available time to gather the large numbers of MS/MS fragmentation spectra required for identification. Orbitrap analyzers scale performance with acquisition time and necessarily sacrifice sensitivity and resolving power to deliver higher acquisition rates. We developed a new mass spectrometer that combines a mass-resolving quadrupole, the Orbitrap, and the novel Asymmetric Track Lossless (Astral) analyzer. The new hybrid instrument enables faster acquisition of high-resolution accurate mass (HRAM) MS/MS spectra compared with state-of-the-art mass spectrometers. Accordingly, new proteomics methods were developed that leverage the strengths of each HRAM analyzer, whereby the Orbitrap analyzer performs full scans with a high dynamic range and resolution, synchronized with the Astral analyzer’s acquisition of fast and sensitive HRAM MS/MS scans. Substantial improvements are demonstrated over previous methods using current state-of-the-art mass spectrometers},
   author = {Hamish I. Stewart and Dmitry Grinfeld and Anastassios Giannakopulos and Johannes Petzoldt and Toby Shanley and Matthew Garland and Eduard Denisov and Amelia C. Peterson and Eugen Damoc and Martin Zeller and Tabiwang N. Arrey and Anna Pashkova and Santosh Renuse and Amirmansoor Hakimi and Andreas Kühn and Matthias Biel and Arne Kreutzmann and Bernd Hagedorn and Immo Colonius and Adrian Schütz and Arne Stefes and Ankit Dwivedi and Daniel Mourad and Max Hoek and Bastian Reitemeier and Philipp Cochems and Alexander Kholomeev and Robert Ostermann and Gregor Quiring and Maximilian Ochmann and Sascha Möhring and Alexander Wagner and André Petker and Sebastian Kanngiesser and Michael Wiedemeyer and Wilko Balschun and Daniel Hermanson and Vlad Zabrouskov and Alexander A. Makarov and Christian Hock},
   doi = {10.1021/acs.analchem.3c02856},
   issn = {15206882},
   issue = {42},
   journal = {Analytical Chemistry},
   title = {Parallelized Acquisition of Orbitrap and Astral Analyzers Enables High-Throughput Quantitative Analysis},
   volume = {95},
   year = {2023}
}
@misc{DITommaso2017,
   author = {Paolo DI Tommaso and Maria Chatzou and Evan W. Floden and Pablo Prieto Barja and Emilio Palumbo and Cedric Notredame},
   doi = {10.1038/nbt.3820},
   issn = {15461696},
   issue = {4},
   journal = {Nature Biotechnology},
   title = {Nextflow enables reproducible computational workflows},
   volume = {35},
   year = {2017}
}
@article{Demichev2020,
   abstract = {We present an easy-to-use integrated software suite, DIA-NN, that exploits deep neural networks and new quantification and signal correction strategies for the processing of data-independent acquisition (DIA) proteomics experiments. DIA-NN improves the identification and quantification performance in conventional DIA proteomic applications, and is particularly beneficial for high-throughput applications, as it is fast and enables deep and confident proteome coverage when used in combination with fast chromatographic methods.},
   author = {Vadim Demichev and Christoph B. Messner and Spyros I. Vernardis and Kathryn S. Lilley and Markus Ralser},
   doi = {10.1038/s41592-019-0638-x},
   issn = {15487105},
   issue = {1},
   journal = {Nature Methods},
   title = {DIA-NN: neural networks and interference correction enable deep proteome coverage in high throughput},
   volume = {17},
   year = {2020}
}
@article{Hulstaert2020,
   abstract = {The field of computational proteomics is approaching the big data age, driven both by a continuous growth in the number of samples analyzed per experiment as well as by the growing amount of data obtained in each analytical run. In order to process these large amounts of data, it is increasingly necessary to use elastic compute resources such as Linux-based cluster environments and cloud infrastructures. Unfortunately, the vast majority of cross-platform proteomics tools are not able to operate directly on the proprietary formats generated by the diverse mass spectrometers. Here, we present ThermoRawFileParser, an open-source, cross-platform tool that converts Thermo RAW files into open file formats such as MGF and the HUPO-PSI standard file format mzML. To ensure the broadest possible availability and to increase integration capabilities with popular workflow systems such as Galaxy or Nextflow, we have also built Conda package and BioContainers container around ThermoRawFileParser. In addition, we implemented a user-friendly interface (ThermoRawFileParserGUI) for those users not familiar with command-line tools. Finally, we performed a benchmark of ThermoRawFileParser and msconvert to verify that the converted mzML files contain reliable quantitative results.},
   author = {Niels Hulstaert and Jim Shofstahl and Timo Sachsenberg and Mathias Walzer and Harald Barsnes and Lennart Martens and Yasset Perez-Riverol},
   doi = {10.1021/acs.jproteome.9b00328},
   issn = {15353907},
   issue = {1},
   journal = {Journal of Proteome Research},
   title = {ThermoRawFileParser: Modular, Scalable, and Cross-Platform RAW File Conversion},
   volume = {19},
   year = {2020}
}
@article{Martens2005,
   abstract = {With the human Plasma Proteome Project (PPP) pilot phase completed, the largest and most ambitious proteomics experiment to date has reached its first milestone. The correspondingly impressive amount of data that came from this pilot project emphasized the need for a centralized dissemination mechanism and led to the development of a detailed, PPP specific data gathering infrastructure at the University of Michigan, Ann Arbor as well as the protein identifications database project at the European Bioinformatics Institute as a general proteomics data repository. One issue that crept up while discussing which data to store for the PPP concerns whether the raw, binary data coming from the mass spectrometers should be stored, or rather the more compact and already significantly processed peak lists. As this debate is not restricted to the PPP but relates to the proteomics community in general, we will attempt to detail the relative merits and caveats associated with centralized storage and dissemination of raw data and/or peak lists, building on the extensive experience gained during the PPP pilot phase. Finally, some suggestions are made for both immediate and future storage of MS data in public repositories. © 2005 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim.},
   author = {Lennart Martens and Alexey I. Nesvizhskii and Henning Hermjakob and Marcin Adamski and Gilbert S. Omenn and Joël Vandekerckhove and Kris Gevaert},
   doi = {10.1002/pmic.200401302},
   issn = {16159853},
   issue = {13},
   journal = {Proteomics},
   title = {Do we want our data raw? Including binary mass spectrometry data in public proteomics data repositories},
   volume = {5},
   year = {2005}
}
@misc{Chambers2012,
   author = {Matthew C. Chambers and Brendan MacLean and Robert Burke and Dario Amodei and Daniel L. Ruderman and Steffen Neumann and Laurent Gatto and Bernd Fischer and Brian Pratt and Jarrett Egertson and Katherine Hoff and Darren Kessner and Natalie Tasman and Nicholas Shulman and Barbara Frewen and Tahmina A. Baker and Mi Youn Brusniak and Christopher Paulse and David Creasy and Lisa Flashner and Kian Kani and Chris Moulding and Sean L. Seymour and Lydia M. Nuwaysir and Brent Lefebvre and Frank Kuhlmann and Joe Roark and Paape Rainer and Suckau Detlev and Tina Hemenway and Andreas Huhmer and James Langridge and Brian Connolly and Trey Chadick and Krisztina Holly and Josh Eckels and Eric W. Deutsch and Robert L. Moritz and Jonathan E. Katz and David B. Agus and Michael MacCoss and David L. Tabb and Parag Mallick},
   doi = {10.1038/nbt.2377},
   issn = {10870156},
   issue = {10},
   journal = {Nature Biotechnology},
   title = {A cross-platform toolkit for mass spectrometry and proteomics},
   volume = {30},
   year = {2012}
}
@misc{Merkel2014,
   abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.},
   author = {Dirk Merkel},
   issn = {1075-3583},
   issue = {239},
   journal = {Linux Journal},
   title = {Docker: lightweight Linux containers for consistent development and deployment},
   volume = {2014},
   year = {2014}
}
@article{Alser2024,
   author = {Mohammed Alser and Brendan Lawlor and Richard J Abdill and Sharon Waymost and Ram Ayyala and Neha Rajkumar and Nathan LaPierre and Jaqueline Brito and André M Ribeiro-dos-Santos and Nour Almadhoun},
   issn = {1754-2189},
   issue = {9},
   journal = {Nature Protocols},
   pages = {2529-2539},
   publisher = {Nature Publishing Group UK London},
   title = {Packaging and containerization of computational methods},
   volume = {19},
   year = {2024}
}
@article{Kong2017,
   abstract = {There is a need to better understand and handle the 'dark matter' of proteomics-the vast diversity of post-translational and chemical modifications that are unaccounted in a typical mass spectrometry-based analysis and thus remain unidentified. We present a fragment-ion indexing method, and its implementation in peptide identification tool MSFragger, that enables a more than 100-fold improvement in speed over most existing proteome database search tools. Using several large proteomic data sets, we demonstrate how MSFragger empowers the open database search concept for comprehensive identification of peptides and all their modified forms, uncovering dramatic differences in modification rates across experimental samples and conditions. We further illustrate its utility using protein-RNA cross-linked peptide data and using affinity purification experiments where we observe, on average, a 300% increase in the number of identified spectra for enriched proteins. We also discuss the benefits of open searching for improved false discovery rate estimation in proteomics.},
   author = {Andy T. Kong and Felipe V. Leprevost and Dmitry M. Avtonomov and Dattatreya Mellacheruvu and Alexey I. Nesvizhskii},
   doi = {10.1038/nmeth.4256},
   issn = {15487105},
   issue = {5},
   journal = {Nature Methods},
   title = {MSFragger: Ultrafast and comprehensive peptide identification in mass spectrometry-based proteomics},
   volume = {14},
   year = {2017}
}
@misc{Chang2025,
   author = {Winston Chang and Joe Cheng and J J Allaire and Carson Sievert and Barret Schloerke and Yihui Xie and Jeff Allen and Jonathan McPherson and Alan Dipert and Barbara Borges},
   note = {R package version 1.10.0.9001},
   title = {shiny: Web Application Framework for R},
   url = {https://shiny.posit.co/},
   year = {2025}
}
@misc{Glish2003,
   abstract = {Enormous advances in our understanding of the chemistry underlying life processes have identified new targets for therapeutic agents. The discovery of effective therapeutics to address these targets is often accomplished through parallel synthetic and screening efforts. In almost all cases, what has enabled target identification and allowed parallel approaches to drug discovery to be effective are the development of either new analytical tools or the improvement of currently existing ones. Among these tools, mass spectrometry has evolved to become an irreplaceable technique in the analysis of biologically related molecules. This article will guide researchers in drug discovery through the basic principles of mass spectrometry.},
   author = {Gary L. Glish and Richard W. Vachet},
   doi = {10.1038/nrd1011},
   issn = {14741776},
   issue = {2},
   journal = {Nature Reviews Drug Discovery},
   title = {The basics of mass spectrometry in the twenty-first century},
   volume = {2},
   year = {2003}
}
@article{Lex2014,
   abstract = {Understanding relationships between sets is an important analysis task that has received widespread attention in the visualization community. The major challenge in this context is the combinatorial explosion of the number of set intersections if the number of sets exceeds a trivial threshold. In this paper we introduce UpSet, a novel visualization technique for the quantitative analysis of sets, their intersections, and aggregates of intersections. UpSet is focused on creating task-driven aggregates, communicating the size and properties of aggregates and intersections, and a duality between the visualization of the elements in a dataset and their set membership. UpSet visualizes set intersections in a matrix layout and introduces aggregates based on groupings and queries. The matrix layout enables the effective representation of associated data, such as the number of elements in the aggregates and intersections, as well as additional summary statistics derived from subset or element attributes. Sorting according to various measures enables a task-driven analysis of relevant intersections and aggregates. The elements represented in the sets and their associated attributes are visualized in a separate view. Queries based on containment in specific intersections, aggregates or driven by attribute filters are propagated between both views. We also introduce several advanced visual encodings and interaction methods to overcome the problems of varying scales and to address scalability. UpSet is web-based and open source. We demonstrate its general utility in multiple use cases from various domains.},
   author = {Alexander Lex and Nils Gehlenborg and Hendrik Strobelt and Romain Vuillemot and Hanspeter Pfister},
   doi = {10.1109/TVCG.2014.2346248},
   issn = {10772626},
   issue = {12},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {UpSet: Visualization of intersecting sets},
   volume = {20},
   year = {2014}
}
@article{Frhlich2024,
   abstract = {Data-independent acquisition (DIA) has revolutionized the field of mass spectrometry (MS)-based proteomics over the past few years. DIA stands out for its ability to systematically sample all peptides in a given m/z range, allowing an unbiased acquisition of proteomics data. This greatly mitigates the issue of missing values and significantly enhances quantitative accuracy, precision, and reproducibility compared to many traditional methods. This review focuses on the critical role of DIA analysis software tools, primarily focusing on their capabilities and the challenges they address in proteomic research. Advances in MS technology, such as trapped ion mobility spectrometry, or high field asymmetric waveform ion mobility spectrometry require sophisticated analysis software capable of handling the increased data complexity and exploiting the full potential of DIA. We identify and critically evaluate leading software tools in the DIA landscape, discussing their unique features, and the reliability of their quantitative and qualitative outputs. We present the biological and clinical relevance of DIA-MS and discuss crucial publications that paved the way for in-depth proteomic characterization in patient-derived specimens. Furthermore, we provide a perspective on emerging trends in clinical applications and present upcoming challenges including standardization and certification of MS-based acquisition strategies in molecular diagnostics. While we emphasize the need for continuous development of software tools to keep pace with evolving technologies, we advise researchers against uncritically accepting the results from DIA software tools. Each tool may have its own biases, and some may not be as sensitive or reliable as others. Our overarching recommendation for both researchers and clinicians is to employ multiple DIA analysis tools, utilizing orthogonal analysis approaches to enhance the robustness and reliability of their findings.},
   author = {Klemens Fröhlich and Matthias Fahrner and Eva Brombacher and Adrianna Seredynska and Maximilian Maldacker and Clemens Kreutz and Alexander Schmidt and Oliver Schilling},
   doi = {https://doi.org/10.1016/j.mcpro.2024.100800},
   issn = {1535-9476},
   issue = {8},
   journal = {Molecular \& Cellular Proteomics},
   keywords = {clinical proteomics,data-independent acquisition,mass spectrometry},
   pages = {100800},
   title = {Data-Independent Acquisition: A Milestone and Prospect in Clinical Mass Spectrometry–Based Proteomics},
   volume = {23},
   url = {https://www.sciencedirect.com/science/article/pii/S1535947624000902},
   year = {2024}
}
@article{Heil2023,
   abstract = {We evaluate the quantitative performance of the newly released Asymmetric Track Lossless (Astral) analyzer. Using data-independent acquisition, the Thermo Scientific Orbitrap Astral mass spectrometer quantifies 5 times more peptides per unit time than state-of-the-art Thermo Scientific Orbitrap mass spectrometers, which have long been the gold standard for high-resolution quantitative proteomics. Our results demonstrate that the Orbitrap Astral mass spectrometer can produce high-quality quantitative measurements across a wide dynamic range. We also use a newly developed extracellular vesicle enrichment protocol to reach new depths of coverage in the plasma proteome, quantifying over 5000 plasma proteins in a 60 min gradient with the Orbitrap Astral mass spectrometer.},
   author = {Lilian R. Heil and Eugen Damoc and Tabiwang N. Arrey and Anna Pashkova and Eduard Denisov and Johannes Petzoldt and Amelia C. Peterson and Chris Hsu and Brian C. Searle and Nicholas Shulman and Michael Riffle and Brian Connolly and Brendan X. MacLean and Philip M. Remes and Michael W. Senko and Hamish I. Stewart and Christian Hock and Alexander A. Makarov and Daniel Hermanson and Vlad Zabrouskov and Christine C. Wu and Michael J. MacCoss},
   doi = {10.1021/acs.jproteome.3c00357},
   issn = {15353907},
   issue = {10},
   journal = {Journal of Proteome Research},
   title = {Evaluating the Performance of the Astral Mass Analyzer for Quantitative Proteomics Using Data-Independent Acquisition},
   volume = {22},
   year = {2023}
}
@article{Reznick1993,
   author = {Larry Reznick},
   issue = {4},
   journal = {Sys Admin},
   pages = {29-32},
   publisher = {Miller Freeman, Inc. Lawrence, KS, USA},
   title = {Using cron and crontab},
   volume = {2},
   year = {1993}
}
@article{Garijo2013,
   abstract = {How easy is it to reproduce the results found in a typical computational biology paper? Either through experience or intuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify this difficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertise to domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimating the time required to reproduce each of the steps in the method described in the original paper and make them part of an explicit workflow that reproduces the original results. Reproducing the method took several months of effort, and required using new versions and new software that posed challenges to reconstructing and validating the results. The quantification leads to "reproducibility maps" that reveal that novice researchers would only be able to reproduce a few of the steps in the method, and that only expert researchers with advance knowledge of the domain would be able to reproduce the method in its entirety. The workflow itself is published as an online resource together with supporting software and data. The paper concludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and a desiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducing the work of others from published papers, but reproducing work from one's own laboratory. © 2013 Garijo et al.},
   author = {Daniel Garijo and Sarah Kinnings and Li Xie and Lei Xie and Yinliang Zhang and Philip E. Bourne and Yolanda Gil},
   doi = {10.1371/journal.pone.0080278},
   issn = {19326203},
   issue = {11},
   journal = {PLoS ONE},
   title = {Quantifying reproducibility in computational biology: The case of the tuberculosis drugome},
   volume = {8},
   year = {2013}
}
@article{Moreau2023,
   abstract = {The fast-paced development of computational tools has enabled tremendous scientific progress in recent years. However, this rapid surge of technological capability also comes at a cost, as it leads to an increase in the complexity of software environments and potential compatibility issues across systems. Advanced workflows in processing or analysis often require specific software versions and operating systems to run smoothly, and discrepancies across machines and researchers can impede reproducibility and efficient collaboration. As a result, scientific teams are increasingly relying on containers to implement robust, dependable research ecosystems. Originally popularized in software engineering, containers have become common in scientific projects, particularly in large collaborative efforts. In this Primer, we describe what containers are, how they work and the rationale for their use in scientific projects. We review state-of-the-art implementations in diverse contexts and fields, with examples in various scientific fields. Finally, we discuss the possibilities enabled by the widespread adoption of containerization, especially in the context of open and reproducible research, and propose recommendations to facilitate seamless implementation across platforms and domains, including within high-performance computing clusters such as those typically available at universities and research institutes.},
   author = {David Moreau and Kristina Wiebels and Carl Boettiger},
   doi = {10.1038/s43586-023-00236-9},
   issn = {26628449},
   issue = {1},
   journal = {Nature Reviews Methods Primers},
   title = {Containers for computational reproducibility},
   volume = {3},
   year = {2023}
}
@article{Perez-Riverol2025,
   author = {Yasset Perez-Riverol and Wout Bittremieux and William S Noble and Lennart Martens and Aivett Bilbao and Michael R Lazear and Bjorn Grüning and Daniel S Katz and Michael J MacCoss and Chengxin Dai and others},
   journal = {Journal of Proteome Research},
   publisher = {ACS Publications},
   title = {Open-Source and FAIR Research Software for Proteomics},
   year = {2025}
}
@article{Baker2016,
   abstract = {A Nature survey lifts the lid on how researchers view the ‘crisis’ rocking science and what they think will help.},
   author = {Monya Baker},
   doi = {10.1038/533452a},
   issn = {0028-0836},
   issue = {7604},
   journal = {Nature},
   title = {1,500 scientists lift the lid on reproducibility},
   volume = {533},
   year = {2016}
}
@misc{Deutsch2008,
   author = {Eric Deutsch},
   doi = {10.1002/pmic.200890049},
   issn = {16159853},
   issue = {14},
   journal = {Proteomics},
   title = {mzML: A single, unifying data format for mass spectrometer output},
   volume = {8},
   year = {2008}
}
@misc{Davis-Turak2017,
   abstract = {Introduction: The emergence and mass utilization of high-throughput (HT) technologies, including sequencing technologies (genomics) and mass spectrometry (proteomics, metabolomics, lipids), has allowed geneticists, biologists, and biostatisticians to bridge the gap between genotype and phenotype on a massive scale. These new technologies have brought rapid advances in our understanding of cell biology, evolutionary history, microbial environments, and are increasingly providing new insights and applications towards clinical care and personalized medicine. Areas covered: The very success of this industry also translates into daunting big data challenges for researchers and institutions that extend beyond the traditional academic focus of algorithms and tools. The main obstacles revolve around analysis provenance, data management of massive datasets, ease of use of software, interpretability and reproducibility of results. Expert commentary: The authors review the challenges associated with implementing bioinformatics best practices in a large-scale setting, and highlight the opportunity for establishing bioinformatics pipelines that incorporate data tracking and auditing, enabling greater consistency and reproducibility for basic research, translational or clinical settings.},
   author = {Jeremy Davis-Turak and Sean M. Courtney and E. Starr Hazard and W. Bailey Glen and Willian A. da Silveira and Timothy Wesselman and Larry P. Harbin and Bethany J. Wolf and Dongjun Chung and Gary Hardiman},
   doi = {10.1080/14737159.2017.1282822},
   issn = {17448352},
   issue = {3},
   journal = {Expert Review of Molecular Diagnostics},
   title = {Genomics pipelines and data integration: challenges and opportunities in the research setting},
   volume = {17},
   year = {2017}
}
@misc{Wratten2021,
   abstract = {The rapid growth of high-throughput technologies has transformed biomedical research. With the increasing amount and complexity of data, scalability and reproducibility have become essential not just for experiments, but also for computational analysis. However, transforming data into information involves running a large number of tools, optimizing parameters, and integrating dynamically changing reference data. Workflow managers were developed in response to such challenges. They simplify pipeline development, optimize resource usage, handle software installation and versions, and run on different compute platforms, enabling workflow portability and sharing. In this Perspective, we highlight key features of workflow managers, compare commonly used approaches for bioinformatics workflows, and provide a guide for computational and noncomputational users. We outline community-curated pipeline initiatives that enable novice and experienced users to perform complex, best-practice analyses without having to manually assemble workflows. In sum, we illustrate how workflow managers contribute to making computational analysis in biomedical research shareable, scalable, and reproducible.},
   author = {Laura Wratten and Andreas Wilm and Jonathan Göke},
   doi = {10.1038/s41592-021-01254-9},
   issn = {15487105},
   issue = {10},
   journal = {Nature Methods},
   title = {Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers},
   volume = {18},
   year = {2021}
}
@article{Dai2024,
   author = {Chengxin Dai and Julianus Pfeuffer and Hong Wang and Ping Zheng and Lukas Käll and Timo Sachsenberg and Vadim Demichev and Mingze Bai and Oliver Kohlbacher and Yasset Perez-Riverol},
   issue = {9},
   journal = {Nature Methods},
   pages = {1603-1607},
   publisher = {Nature Publishing Group US New York},
   title = {quantms: a cloud-based pipeline for quantitative proteomics enables the reanalysis of public proteomics data},
   volume = {21},
   year = {2024}
}
@article{Ewels2020,
   author = {Philip A Ewels and Alexander Peltzer and Sven Fillinger and Harshil Patel and Johannes Alneberg and Andreas Wilm and Maxime Ulysse Garcia and Paolo Di Tommaso and Sven Nahnsen},
   doi = {10.1038/s41587-020-0439-x},
   issn = {1546-1696},
   issue = {3},
   journal = {Nature Biotechnology},
   pages = {276-278},
   title = {The nf-core framework for community-curated bioinformatics pipelines},
   volume = {38},
   url = {https://doi.org/10.1038/s41587-020-0439-x},
   year = {2020}
}
