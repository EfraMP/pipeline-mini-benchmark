---
title: " Mini-benchamrking of a Nextlfow pipeline for MS-based proteomics"
author: "Alejandro Efraín Marín Peralta"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    code_folding: hide
    toc: true
    toc_float:
      toc_collapsed: true
    number_sections: true
    toc_depth: 3
    fig_caption: true
    theme: lumen
bibliography: ./cites/export.bib
csl: ./cites/nature.csl
---


```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
# Env set up
## Libraries
library(tidyverse)
library(arrow)
library(magrittr)
library(fs)
library(ggbeeswarm)
library(UpSetR)
library(arrow)
library(pheatmap)
library(viridis)
library(paletteer)
library(diann)
library(biomaRt)
library(clusterProfiler)
library(org.Hs.eg.db)

## Colors for organisms
org_cols <- c(
  Ecoli    = "#007B53",
  Hsapiens = "#193F90"
)

## Overwrites all functions
select <- dplyr::select

## Extra set uo
set.seed(0)
setwd("~/Documents/EMBL_UniHD/mini-benchmark")

## Sources functions
source("scripts/helpers.r")
```


# Introduction
 
**Proteomics** is the quantitative study of the proteome, *i. e.*, the whole set of proteins available in a biological sample @Sinha2020. **Mass spectrometry** (MS) is the current state-of-the-art method to do proteomics analyses @Shuken2023. MS consists of measuring the **mass-to-charge ratio** ($m/z$) of ions @Glish2003.  MS tends to be combined with high-performance liquid chromatography (HPLC)@Sinha2020. Thus, in liquid chromatography-mass spectrometry (LC-MS), ions tend to be identified based on both their chromatographic retention time and $m/z$. In the case of MS-based proteomics, the ions to be analyzed are ionized peptides generated from enzymatic digestion @Sinha2020. 

Proteomics mass spectrometry consists of two steps. First, intact peptide ions (also called **precursors**) generate a first MS1 spectrum. Then a subset of them is subjected into **fragmentation**, generating a MS/MS spectrum. The fragmentation tends to break peptide bonds, thus the MS/MS aids into the identification of the peptide sequence @Shuken2023. 

There are two main data acquisition and quantification strategies: data-dependent acquisition (DDA) and data-independent acquisition (DIA),@Sinha2020 as shown in Figure \@ref(fig:data-acq). With the DDA strategy, a precursor is selected from MS1 at a given retention time to go into MS/MS. On the other hand, with the DIA strategy, multiple precursors are selected from a $m/z$ window of a given retention time to go into MS/MS.


```{r data-acq, echo=FALSE, fig.cap="Data acquisition strategies: DDA (A) and DIA (B). (Sinha & Mann, 2020)", fig.align='center', out.width='80%'}
knitr::include_graphics("figures/dda_vs_dia.png")
```


The usage of DIA seems promising, as it allows the identification and quantification of peptide in an unbiased way @Frhlich2024. Besides, with the introduction of the new *Thermo Fisher Scientific Inc.*® instrument, the Orbitrap Astral mass spectrometer @Stewart2023, which is capable to quantify 5 times more peptides per unit time compared to other mass spectrometers compared to other DIA-based instruments @Heil2023, the area of proteomics seems to be advancing rapidly.

Nevertheless, there a lot of challenges in the computational aspect of the proteomics field, such as the data size and complexity, the prevalence of closed source software and restrictive licenses, the lack of good documentation, the lack of standardization in proteomics software development and the lack of maintenance of open source software @Perez-Riverol2025. For example, the mass spectrometers raw output files are in proprietary, licensed, closed and binary formats @Martens2005. Given the restrictive-access nature of these files, an open standard was needed, which is why the mzML @Deutsch2008 file format was created.

Another well-known problem in current science is the reproducibility crisis @Baker2016. Specifically, **computational reproducibility** is achieved when the same input data, code and software environment produce exactly the same output @Moreau2023. Nevertheless, this is not necessarily true across different computational platforms (i. e., different operating systems) due to intrinsic numerical instability @DITommaso2017. To address this issue, the use of containers is a popular solution in modern scientific computing @Moreau2023. Containers are a self-contained executable package isolated from the host system @Alser2024. Currently, the most used containerization technology is Docker @Merkel2014.

On the other hand, bioinformatics analyses are complex, multi-step processes that require the use of multiple specialized software tools in a chained sequence @Davis-Turak2017. Besides the reproducibility issue previously discussed, there are more challenges that the field is facing such as: incompatibilities between software, manual software execution steps, high complexity and file size of omics data, among others @Wratten2021. To tackle these problems, Nextflow @DITommaso2017 was developed both as a scientific workflow management system and domain specific language (DSL).

To assess this challenges, a Nextflow-based pipeline was to analyze both DDA and DIA proteomics data.
The objective of this notebook is to benchmark the DIA-specific subworfklows (/@ref(fig:metromap)). The tools integrated into the subworkflow created to convert files from raw to mzML format were MSConvert @Chambers2012 and ThermoRawFileParser @Hulstaert2020. For DIA, DIA-NN @Demichev2020 processes and workflows were implemented. Docker images were built for all the DIA-NN versions available for Linux at the moment of the pipeline development (1.8.1, 1.9.1, 1.9.2, 2.0, 2.0.1, 2.0.2 and 2.1). Given incompatibilities across different DIA-NN versions, specific subworkflows for DIA-NN v1.8.1 and DIA-NN v2.1 were coded. Note that DIA-NN v2.1 accepts raw files as input, making the mzML conversion an unnecessary step. The DIA-NN steps are integrated as different Nextflow processes. In the end, the pipeline was built as modular as possible.


```{r metromap, echo=FALSE, fig.cap="Pipeline DIA-NN subworkflows metromap", fig.align='center', out.width='90%'}
knitr::include_graphics("figures/metromap.png")
```


DIA-NN is meant to be run in a multi-step pipeline following the next two steps:

* *In silico* spectral library prediction given a FASTA file with protein sequences, DIA-NN predicts a spectral library, which is a set of spectra with annotated information such as retention time and *m/z*.
* Analysis: DIA-NN analyze the raw data with the spectral library generated in the previous step.

Custom Docker images were created individually for most of the pipeline processes; except for the MSConvert conversion (`proteowizard/pwiz-skyline-i-agree-to-the-vendor-licenses`) and the ThermoRawFileParser conversion (`quay.io/biocontainers/thermorawfileparser:1.4.5--h05cac1d_1`). 

To benchmark the pipeline, samples from *Escherichia coli* and *Homo sapiens* were processed and measured by in three different mass spectrometer instruments: FusionLumos, Exploris480 and Astral. Three replicates were used per each combination (*i.e.* different mass spectrometers and the three species), yielding 18 different raw files. Different pipeline profiles were run considering the DIA-NN versions and raw file files processing strategies in a combinatorial way.

To assess the computational efficiency of the pipeline runs, different metrics were considered and contrasted, such as the pipeline total runtime and the size of the input files.  In order to address the pipeline performance regarding biological metrics, the precursor identification and quantification results were measured.



# Computing insights


```{r comp_functions, include=FALSE}
# # Auxiliary functions to convert units
# 
# ## Function to convert units to MB
# get_MB <- function(x) {
#   ## Trim whitespace
#   x %<>% as.character() %>% 
#     trimws()
#   
#   ## Gets numeric value
#   val <- str_extract(x, "[0-9]+(\\.[0-9]+)?") %>% 
#     as.numeric()
#   
#   ## Gets unit
#   unit <- str_extract(x, "[A-Za-z]+") %>% 
#     toupper()
#   
#   ## Gets MB
#   value_mb = case_when(
#     unit %in% c("KB", "K")  ~ val / 1024,
#     unit %in% c("MB", "M")  ~ val,
#     unit %in% c("GB", "G") ~ val * 1024,
#     unit %in% c("TB", "T") ~ val * 1024 * 1024,
#     TRUE ~ NA_real_
#   )
#   
#   ## Return megabytes
#   return(value_mb)
# }
# 
# 
# ## Function to get time information into seconds
# get_seconds <- function(x) {
#   x %<>% as.character()
#   
#   ## Gets time with regex
#   h <- x %>% 
#     str_extract(., "\\d+(?=h)") %>% 
#     as.numeric()
#   m <- x %>% 
#     str_extract(., "\\d+(?=m)") %>% 
#     as.numeric()
#   ## Regex to allow decimal seconds
#   s <- x %>% 
#     str_extract(., "\\d+(?:\\.\\d+)?(?=s)") %>% 
#     as.numeric()
#   
#   # Replace NAa with 0
#   h[is.na(h)] <- 0
#   m[is.na(m)] <- 0
#   s[is.na(s)] <- 0
#   
#   ## Return seconds
#   return(h * 3600 + m * 60 + s)
# }


```


Gets metadata from server after running
`ls -lh */*/data/*raw | tr -s ' ' '\t' > files_list.tsv` and then secure copying
it to the local root.

```{r raw_info, message=FALSE}
raw_info <- read_tsv("./files_list.tsv",
                     col_names = FALSE)

head(raw_info)


## Keeps just the necessary information
raw_info %<>% dplyr::select(c("X5", "X9")) %>% 
  purrr::set_names(c("size", "fullname"))

## Mutates the tibble
raw_info %<>% mutate(size_MB = get_MB(size)) %>% 
  separate(
    fullname,
    into = c("instrument", "organism", "subfolder", "file_name"),
    sep = "/"
  )

head(raw_info)


### Plot
ggplot(raw_info, aes(x = instrument, y = size_MB / 1024, colour = organism)) +
  geom_beeswarm(size = 2, alpha = 0.7) +
  scale_x_discrete(limits = c("FusionLumos", "Exploris480", "Astral")) +
  scale_colour_manual(
    name = "Organism",
    values = org_cols
  ) +
  theme_minimal() +
  ylim(0, 12.5) +
  labs(
    x = "Mass Spectrometer",
    y = "Raw file size (GB)"
  ) +
  ggtitle("Size of raw files per experiment")




### Median size by instrument
raw_info %>% 
  group_by(instrument) %>% 
  summarise(median(size_MB / 1024))



### Median size by organism
raw_info %>% 
  group_by(organism) %>% 
  summarise(median(size_MB / 1024))


```


```{r summarize_raw}
## Collapse into average size of raw files
raw_summary <- raw_info %>%
  group_by(instrument, organism) %>%
  summarise(
    avg_raw_size = mean(size_MB, na.rm = TRUE),
    ## Cleans unnecessary metadata
    .groups  = "drop"
  ) %>% 
  ## Converts back to GB
  mutate(avg_raw_size = avg_raw_size / 1024)
```


```{r fasta_info}
fasta_info <- read_tsv("./fasta_list.tsv",
                       col_names = FALSE)


fasta_info

## Filter out unnecessary columns
fasta_info %<>% dplyr::select(c("X5", "X9")) %>% 
  purrr::set_names(c("size", "fullname")) %>% 
  mutate(size_MB = get_MB(size)) %>% 
  ## Get organism name with regex
  mutate(organism = str_extract(fullname, "(?<=/)[^/_]+(?=_)"))

```



Get times after doing `grep -m1 'duration: <strong>' */*/*/nf_report.html > runtimes.txt`

```{r runtimes, message=FALSE}
runtimes <- read_delim("runtimes.txt",
                       delim =  "          ", ## By some reason grep created 10 whitespaces
                       col_names = FALSE) %>% 
  set_names(c("run", "string")) %>% 
  mutate(time = get_seconds(string) / 60,
         instrument = word(run, 1, sep = "/"),
         organism = word(run, 2, sep = "/"),
         profile = word(run, 3, sep = "/")) %>% 
  select(!run) %>% 
  select(!string)

### 
runtimes %>% 
  filter(!str_detect(profile, "1\\.9\\.1")) %>% 
  ggplot() +
  geom_beeswarm(aes(y = time / 60, x = instrument, colour = organism),
                size = 2,
                alpha = 0.8,
                ) +
  scale_colour_manual(
    name = "Organism",
    values = org_cols
  ) +
  geom_boxplot(aes(y = time / 60, x = instrument),
               width = .25, alpha = 0, 
               outlier.shape = NA) +
  scale_x_discrete(limits = c("FusionLumos", "Exploris480", "Astral")) +
  theme_minimal() +
  labs(
    x =  "Mass Spectrometer",
    y = "Total runtime (h)",
    title = "Runtime per experiment"
  )


### Median runtime per instrument
runtimes %>%
  filter(!str_detect(profile, "1\\.9\\.1")) %>%
  group_by(instrument) %>%
  summarise(median(time))


### Median runtime per organism
runtimes %>%
  filter(!str_detect(profile, "1\\.9\\.1")) %>%
  group_by(organism) %>%
  summarise(median(time))



size_time <- left_join(runtimes, raw_summary,
                       by = c("instrument", "organism"))



size_time %>% 
  filter(!str_detect(profile, "1\\.9\\.1")) %>% 
  ggplot(aes(x = avg_raw_size, y = time /60 ,
                        colour = organism, 
                        shape = instrument)) +
  geom_point(size = 3, alpha = 0.8) +
  scale_colour_manual(
    name = "Organism",
    values = org_cols
  ) +
  theme_minimal() +
  labs(
    x = "Average raw file size (GB)",
    y = "Total runtime (h)",
    title = "Runtime vs file size"
  )



size_time %>%
  filter(!str_detect(profile, "1\\.9\\.1")) %>% 
  filter(instrument == "Astral") %>% 
  ggplot(aes(x = avg_raw_size, y = time /60 ,
             colour = organism)) +
  geom_point(size = 2, alpha = 0.8) +
  scale_colour_manual(
    name = "Organism",
    values = org_cols
  ) +
  theme_minimal() +
  labs(
    x = "Average raw file size (GB)",
    y = "Total runtime (h)" 
  ) +
  ggtitle("Astral runs")



size_time %>%
  filter(!str_detect(profile, "1\\.9\\.1")) %>% 
  filter(instrument == "Exploris480") %>% 
  ggplot(aes(x = avg_raw_size, y = time /60 ,
             colour = organism)) +
  geom_point(size = 2, alpha = 0.8) +
  scale_colour_manual(
    name = "Organism",
    values = org_cols
  ) +
  theme_minimal() +
  labs(
    x = "Average raw file size (GB)",
    y = "Total runtime (h)" 
  ) +
  ggtitle("Exploris480 runs")



size_time %>%
  filter(!str_detect(profile, "1\\.9\\.1")) %>% 
  filter(instrument == "FusionLumos") %>% 
  ggplot(aes(x = avg_raw_size, y = time /60 ,
             colour = organism)) +
  geom_point(size = 2, alpha = 0.8) +
  scale_colour_manual(
    name = "Organism",
    values = org_cols
  ) +
  theme_minimal() +
  labs(
    x = "Average raw file size (GB)",
    y = "Total runtime (h)" 
  ) +
  ggtitle("FusionLumos runs")


```





```{r deconv}
deconv_rt <- function(x, ms, org) {
  x %>% 
    filter(
      !str_detect(profile, "1\\.9\\.1"),
      instrument == ms,
      organism == org
    ) %>% 
    mutate(
      convert_tool = word(profile, 1, sep = "_"),
      diann_version = word(profile, 2, sep = "_"),
    ) %>% 
    mutate(
      diann_version = replace_na(diann_version, "diannv2.1")
    ) %>% 
    ggplot(aes(x = convert_tool, y = time / 60, shape = diann_version)) +
    geom_beeswarm(size = 3, color = "#18974C") +
    theme_minimal() +
    labs(
      x = "Raw conversion tool",
      y = "Total runtime (h)"
    ) +
    ggtitle(paste("Pipeline runtimes for", org, "measured in", ms))
}


deconv_rt(size_time, "Astral", "Hsapiens") +
  ylim(0, NA)


deconv_rt(size_time, "Astral", "Ecoli")  +
  ylim(0, NA)

```





```{r read_traces, message=FALSE}
## Gets names of the traces TSVs
traces_files <- dir_ls(
  path = ".",
  recurse = TRUE,
  glob = "*nf_trace.tsv"
)

## Reads all the trace files
traces_list <- traces_files %>% 
  purrr::set_names() %>% 
  map(read_tsv)
```



```{r traces, warning=FALSE}
## Using map because it is a list of tibbles
traces_list %<>% map(
  ~ mutate(
    .x,
    
    ### Convert time metrics to seconds
    duration_seconds = get_seconds(duration),
    realtime_seconds = get_seconds(realtime),
    
    ### Convert memory metrics to MB
    peak_rss_MB = get_MB(peak_rss),
    peak_vmem_MB = get_MB(peak_vmem),
    rchar_MB = get_MB(rchar),
    wchar_MB = get_MB(wchar),
    
    ### Get CPU percentage
    cpu_percentage = .x %>% 
      pluck("%cpu") %>% 
      str_remove("%") %>% 
      as.numeric()
  )
)
  

## Adds metadata from the tibble name as column
## Use imap() instead if map2() for simplicity
traces_list %<>% imap(~ .x %>% 
                        mutate(
                          instrument = word(.y, 1, sep = "/"),
                          organism = word(.y, 2, sep = "/"),
                          profile = word(.y, 3, sep = "/")
                        ))


traces_list[1] %>% str()
```


summarize everything


```{r traces_times}
## Summarize total times per <instrument/organism/profile>
traces_times <- imap_dfr(traces_list, ~ {
  # split the list‐name (.y) into components
  comb <- str_split(.y, "/", simplify = TRUE)
  tibble(
    instrument = comb[1],
    organism = comb[2],
    profile = comb[3],
    ## Also convert to minutes
    total_duration_min = sum(.x$duration_seconds, na.rm = TRUE) %>% { . / 60 },
    total_realtime_min = sum(.x$realtime_seconds, na.rm = TRUE) %>% { . / 60 }
  )
})
```



# Biological insights

## Quality Control

```{r qc_functions}
## Standard QC thresholds on DIA-NN reports
## Extracts high quality peptides
qc <- function(x) {
  x %>% 
    collect() %>% 
    filter(
      ## Standard thresholds
      Q.Value <= 0.01,
      Lib.Q.Value <= 0.01,
      Lib.PG.Q.Value <= 0.01,
      PG.Q.Value <= 0.05,
      ### Filters out contaminants
      !str_detect(Protein.Group, "contam_sp"),
      !str_detect(Protein.Ids, "contam_sp")
    ) %>% 
    pull(Precursor.Id) %>%
    unique()
}


## Function to perform qc on the report's precursor
## and filter them out from the matrices
clean <- function(report, mat) {
  out <- mat %>% 
    filter(Precursor.Id %in% qc(report)) %>%
    ## Collapses to precursor level and removes missing values
    mutate(
      ## Converts NAs to 0s
      across(c(ends_with("mzML"), ends_with("raw")),
             ~ replace_na(.x, 0))
    ) %>% 
    group_by(Precursor.Id) %>% 
    mutate(
      ## Sum intensities per precursors
      across(c(ends_with("mzML"), ends_with("raw")),
             sum),
    ) %>% 
    ungroup() %>% 
    filter(
      ## Filters our zeros
      !if_any(c(ends_with("mzML"), ends_with("raw")),
              ~. == 0)
    )
}
```



```{r read_diann_outputs, message=FALSE, warning=FALSE}
## Lists parquets
parquet_list <- dir_ls(
  path = ".",
  recurse = TRUE,
  glob = "*report.parquet"
) %>% 
  ## Reads all the parquet files
  purrr::set_names() %>% 
  map(open_dataset, format = "parquet")


## Lists DIA-NN v1.8 reports
diann_list <- dir_ls(
  path = ".",
  recurse = TRUE,
  glob = "*diann-report"
) %>% 
  ## Read DIA-NN v1.8 reports
  purrr::set_names() %>% 
  map(diann_load)


## Concatenates all
all_reports <- c(
  parquet_list,
  diann_list
)


## Lists matrices
pr_matrixes <- dir_ls(
  path = ".",
  recurse = TRUE,
  regexp = "*pr_matrix.tsv"
) %>% 
  ## Exclude firts passed
  str_subset("first-pass", negate = TRUE) %>% 
  purrr::set_names() %>% 
  map(read_tsv)
```



```{r qc}
# Performs QC

## Gets <instrument/organism/profile> keys
keys <- all_reports %>% 
  names() %>% 
  str_replace("^((?:[^/]+/){2}[^/]+).*", "\\1")


keyed_reports <- all_reports %>% 
  set_names(
    sub("^((?:[^/]+/){2}[^/]+).*", "\\1", names(.) )
  )


keyed_matrices <- pr_matrixes %>% 
  set_names(
    sub("^((?:[^/]+/){2}[^/]+).*", "\\1", names(.) )
  )

pr_matrixes_cleaned <- imap(
  keyed_reports,
  ~ if(.y %in% names(keyed_matrices)) 
    clean(.x, keyed_matrices[[.y]])
)


```




## Detected biological entities

```{r bio_functions}
# Functions to extract information from DIA-NN outputs

## Get protein groups from parquet DIA-NN output
pg <- function(x) {
  x %>% 
    collect() %>% 
    pull(Protein.Group) %>% 
    unique()
}

## Get proteins from parquet DIA-NN output
proteins <- function(x) {
  x %>% 
    collect() %>% 
    pull(Protein.Ids) %>% 
    unique()
}

## Get peptides from parquet DIA-NN output
peptides <- function(x) {
  x %>% 
    collect() %>% 
    pull(Precursor.Id) %>% 
    unique()
}
```



```{r parquets}
pg_list <- c(
  pr_matrixes_cleaned %>% map(pg)
  )


proteins_list <- c(
  pr_matrixes_cleaned %>% map(proteins)
)


peptides_list <- c(
  pr_matrixes_cleaned %>% map(peptides)
)
```



```{r jaccard_index}
# Auxiliary functions to get Jaccard indexes 

## Computes Jaccard Index
jaccard <- function(x, y) {
  length(intersect(x, y)) / length(union(x, y))
}


## Commputes jaccard index pairwise of list
jpw <- function(x) {
  n <- names(x)
  ## Expands pairwise
  expand_grid(
    names1 = n,
    names2 = n,
  ) %>% 
    ## Maps the function fer columns
    mutate(
      index = map2_dbl(x[names1], x[names2], jaccard)
    ) %>% 
    ## Comes back to squared datafra,e
    pivot_wider(
      names_from = names2,
      values_from = index
    ) %>%
    column_to_rownames("names1") 
}

## Function to automatize heatmap plotting
jaccard_hm <- function(x, string) {
  x %>% 
    jpw() %>% 
      pheatmap(
        main  = string,
        cluster_rows = TRUE,
        cluster_cols = TRUE,
        border_color = NA,
        color = viridis(100),
        angle_col = 45,
        fontsize_row = 10,
        fontsize_col = 10,
        cellwidth  = 30,
        cellheight = 30
      )
}


## Function to automatize upset plotting
jaccard_us <- function(x) {
  x %>% 
    fromList() %>% 
    upset(nsets = ncol(.),
          order.by = "freq",
          decreasing = TRUE,
          nintersects = 10)
}


```


### Protein groups 


#### *Homo sapiens*

```{r pg_human_astral}
pg_hsapiens <- pg_list %>% 
  { .[str_detect(names(.), "Hsapiens")] }


pg_hsapiens_astral <- pg_hsapiens %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pg_hsapiens_astral, "Astral H sapiens | Pairwise Jaccard Index of identified protein groups")

jaccard_us(pg_hsapiens_astral)
```




```{r pg_hsapiens_exploris}
pg_hsapiens_exploris <- pg_hsapiens %>% 
  { .[str_detect(names(.), "Exploris480")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pg_hsapiens_exploris, "Exploris480 H sapiens | Pairwise Jaccard Index")

jaccard_us(pg_hsapiens_exploris)
```




```{r pg_hsapiens_lumos}
pg_hsapiens_lumos <- pg_hsapiens %>% 
  { .[str_detect(names(.), "FusionLumos")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pg_hsapiens_lumos, "FusionLumos H sapiens | Pairwise Jaccard Index")

jaccard_us(pg_hsapiens_lumos)
```



#### *Escherichia coli*


```{r pg_ecoli_astral}
pg_ecoli <- pg_list %>% 
  { .[str_detect(names(.), "Ecoli")] }


pg_ecoli_astral <- pg_ecoli %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pg_ecoli_astral, "Astral E coli | Pairwise Jaccard Index")

jaccard_us(pg_ecoli_astral)
```




```{r pg_ecoli_exploris}
pg_ecoli_exploris <- pg_ecoli %>% 
  { .[str_detect(names(.), "Exploris480")] } %>% 
  set_names( word(names(.), 3, sep = "/") )

jaccard_hm(pg_ecoli_exploris, "Exploris480 E coli | Pairwise Jaccard Index")

jaccard_us(pg_ecoli_exploris)
```




```{r pg_ecoli_lumos}
pg_ecoli_lumos <- pg_ecoli %>% 
  { .[str_detect(names(.), "FusionLumos")] } %>% 
   set_names( word(names(.), 3, sep = "/") )

jaccard_hm(pg_ecoli_lumos, "FusionLumos E coli | Pairwise Jaccard Index")
jaccard_us(pg_ecoli_lumos)

```


### Proteins



#### *Homo sapiens*

```{r prots_human_astral}
prots_hsapiens <- proteins_list %>% 
  { .[str_detect(names(.), "Hsapiens")] }


prots_hsapiens_astral <- prots_hsapiens %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(prots_hsapiens_astral, "Astral H sapiens | Pairwise Jaccard Index of identified proteins")

jaccard_us(prots_hsapiens_astral)
```




```{r prots_hsapiens_exploris}
prots_hsapiens_exploris <- prots_hsapiens %>% 
  { .[str_detect(names(.), "Exploris480")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(prots_hsapiens_exploris, "Exploris480 H sapiens | Pairwise Jaccard Index")

jaccard_us(prots_hsapiens_exploris)
```




```{r prots_hsapiens_lumos}
prots_hsapiens_lumos <- prots_hsapiens %>% 
  { .[str_detect(names(.), "FusionLumos")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(prots_hsapiens_lumos, "FusionLumos H sapiens | Pairwise Jaccard Index")

jaccard_us(prots_hsapiens_lumos)
```



#### *Escherichia coli*


```{r prots_ecoli_astral}
prots_ecoli <- pg_list %>% 
  { .[str_detect(names(.), "Ecoli")] }


prots_ecoli_astral <- prots_ecoli %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(prots_ecoli_astral, "Astral E coli | Pairwise Jaccard Index")

jaccard_us(prots_ecoli_astral)
```




```{r prots_ecoli_exploris}
prots_ecoli_exploris <- prots_ecoli %>% 
  { .[str_detect(names(.), "Exploris480")] } %>% 
  set_names( word(names(.), 3, sep = "/") )

jaccard_hm(prots_ecoli_exploris, "Exploris480 E coli | Pairwise Jaccard Index")

jaccard_us(prots_ecoli_exploris)
```




```{r prots_ecoli_lumos}
prots_ecoli_lumos <- prots_ecoli %>% 
  { .[str_detect(names(.), "FusionLumos")] } %>% 
   set_names( word(names(.), 3, sep = "/") )

jaccard_hm(prots_ecoli_lumos, "FusionLumos E coli | Pairwise Jaccard Index")

jaccard_us(prots_ecoli_lumos)
```



### Precursors



#### *Homo sapiens*

```{r pepts_hsapiens_astral}
pepts_hsapiens <- peptides_list %>% 
  { .[str_detect(names(.), "Hsapiens")] }


pepts_hsapiens_astral <- pepts_hsapiens %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pepts_hsapiens_astral, "Astral H sapiens | Pairwise Jaccard Index of identified precursors")

jaccard_us(pepts_hsapiens_astral)
```




```{r pepts_hsapiens_exploris}
pepts_hsapiens_exploris <- pepts_hsapiens %>% 
  { .[str_detect(names(.), "Exploris480")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pepts_hsapiens_exploris, "Exploris480 H sapiens | Pairwise Jaccard Index")

jaccard_us(pepts_hsapiens_exploris)
```




```{r pepts_hsapiens_lumos}
pepts_hsapiens_lumos <- pepts_hsapiens %>% 
  { .[str_detect(names(.), "FusionLumos")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pepts_hsapiens_lumos, "FusionLumos H sapiens | Pairwise Jaccard Index")

jaccard_us(pepts_hsapiens_lumos)
```





#### *Escherichia coli*


```{r pepts_ecoli_astral}
pepts_ecoli <- peptides_list %>% 
  { .[str_detect(names(.), "Ecoli")] }


pepts_ecoli_astral <- pepts_ecoli %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  set_names( word(names(.), 3, sep = "/") )


jaccard_hm(pepts_ecoli_astral, "Astral E coli | Pairwise Jaccard Index")

jaccard_us(pepts_ecoli_astral)
```




```{r pepts_ecoli_exploris}
pepts_ecoli_exploris <- pepts_ecoli %>% 
  { .[str_detect(names(.), "Exploris480")] } %>% 
  set_names( word(names(.), 3, sep = "/") )

jaccard_hm(pepts_ecoli_exploris, "Exploris480 E coli | Pairwise Jaccard Index")

jaccard_us(pepts_ecoli_exploris)
```




```{r pepts_ecoli_lumos}
pepts_ecoli_lumos <- pepts_ecoli %>% 
  { .[str_detect(names(.), "FusionLumos")] } %>% 
   set_names( word(names(.), 3, sep = "/") )

jaccard_hm(pepts_ecoli_lumos, "FusionLumos E coli | Pairwise Jaccard Index")

jaccard_us(pepts_ecoli_lumos)
```








## Coefficient of variation


```{r cv}
## Function to compute cross validations
compute_cv <- function(x) {
  x %>% 
    rowwise() %>% 
    mutate(
      cv = {
        c_across(c(ends_with("mzML"), ends_with("raw"))) %>% 
          log() %>% 
          { sd(.)/mean(.) }
      }
    ) %>% 
    ungroup()
}

## 


#pr_matrixes_cleaned %<>% lapply(compute_cv)
```




```{r cvs_astral}
## Gets the common peptides detected across all profiles
common_peptds <- purrr::reduce(pepts_hsapiens_astral, intersect)

## Gets data just from Astral and Human
prs_hsapiens_astral <- pr_matrixes_cleaned %>% 
  { .[str_detect(names(.), "Hsapiens")] } %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  ## Just common peptides
  lapply(filter, Precursor.Id %in% common_peptds) %>% 
  ## Compute cvs
  lapply(compute_cv)
  ## Compute mean
  #lapply(compute_means)



## Extracts cvs
cvs <- prs_hsapiens_astral %>% 
  ## Creates tibble with sources and all cv's
  imap_dfr(
    ~ tibble(
      profile = .y,
      cv     = .x$cv
    )
  ) %>% 
  mutate(
    ## Deletes unnecessary info
    profile = word(profile, 3, sep = "/")
  )
```



```{r cv_plots}
## Violin and boxplot
ggplot(cvs, aes(x = profile, y = cv)) +
  geom_violin(color = "gray",
              fill = "#6CC24A",
              alpha = 0.5) +
  geom_boxplot(fill = "#18974C",
               width = .25,
               outlier.shape = NA) +
  theme_minimal() +
  scale_y_sqrt() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45,  hjust = 1, vjust = 1)) +
  labs(
    x = "Workflow",
    y = "Coefficient of Variation",
    title = "CV's across different workflows | Astral, H sapiens"
  )


## Just boxplot
ggplot(cvs, aes(x = profile, y = cv)) +
  geom_boxplot(fill = "#18974C",
               width = .5,
               outlier.shape = NA) +
  ylim(0, 0.035) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  labs(
    x = "Workflow",
    y = "Coefficient of Variation",
    title = "CV's across different workflows | Astral, H sapiens"
  )


## Stats plots
cvs %>%
  group_by(profile) %>%
  summarise(mean_cv = mean(cv)) %>%
  ggplot(aes(x = reorder(profile, mean_cv), y = mean_cv)) +
  geom_col(fill = "#18974C", color = "black") +
  labs(title = "Mean CV per workflow",
       x = "Profile",
       y = "Mean CV") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))


cvs %>%
  group_by(profile) %>%
  summarise(median_cv = median(cv)) %>%
  ggplot(aes(x = reorder(profile, median_cv), y = median_cv)) +
  geom_col(fill = "#18974C", color = "black") +
  labs(title = "Median CV per workflow",
       x = "Profile",
       y = "Median CV") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))


cvs %>%
  group_by(profile) %>%
  summarise(sd_cv = sd(cv)) %>%
  ggplot(aes(x = reorder(profile, sd_cv), y = sd_cv)) +
  geom_col(fill = "#18974C", color = "black") +
  labs(title = "Standard Deviation CV per workflow",
       x = "Profile",
       y = "SD CV") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))

```






## Intensities of unidentified precursors


```{r ints_unid}
### All with peptides identified with newer DIA-NN versions
list_pepts_no_diann1_8 <- pepts_hsapiens_astral[c(
  "diannv2.1",
  "msconvert_diannv1.9.2",
  "msconvert_diannv2.0",
  "msconvert_diannv2.0.1",
  "msconvert_diannv2.0.2",
  "thermoraw_diannv1.9.2",
  "thermoraw_diannv2.0",
  "thermoraw_diannv2.0.1",
  "thermoraw_diannv2.0.2"
)]

## Intersection off all sets
pepts_no_diann1_8 <- Reduce(intersect, list_pepts_no_diann1_8)

## Intersection of set of dia-nn 1.8
pepts_diann1_8 <- c(
  pepts_hsapiens_astral$msconvert_diannv1.8.1,
  pepts_hsapiens_astral$thermoraw_diannv1.8.1
)

## Proteins identified by all versions but DIA-NN 1.8
pepts_unid <- setdiff(pepts_no_diann1_8, pepts_diann1_8)


## Flags if the precursor was identified by DIA-NN 1.8 or not in the precrusos matrix
all_prs_mat <- pr_matrixes_cleaned %>% 
  { .[str_detect(names(.), "Hsapiens")] } %>% 
  { .[str_detect(names(.), "Astral")] } %>% 
  map(
  ~.x %>% 
    mutate(is_unid = Precursor.Id %in% pepts_unid)
)

## Pivots everythin into a single object
all_long <- map_dfr(
  all_prs_mat,
  ~ .x %>%
    pivot_longer(
      cols = c(ends_with("mzML"), ends_with("raw")),
      names_to = "sample",
      values_to = "intensity"
    ),
  .id = "tibble_id"
)


## Plots all densities of all samples together
ggplot(all_long, aes(x = log(intensity),
                     color = !(is_unid),
                     group = interaction(sample, !(is_unid)))) +
  geom_density(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Intensity densities of all Astral H. sapiens runs",
    x = "log(Intensity)",
    y = "Density",
    color = "Identified by DIA-NN 1.8"
  )


```



## GO enrichment of unidentifed proteins


```{r go_enrich}
### All with proteins identified with newer DIA-NN versions
list_prots_no_diann1_8 <- prots_hsapiens_astral[c(
  "diannv2.1",
  "msconvert_diannv1.9.2",
  "msconvert_diannv2.0",
  "msconvert_diannv2.0.1",
  "msconvert_diannv2.0.2",
  "thermoraw_diannv1.9.2",
  "thermoraw_diannv2.0",
  "thermoraw_diannv2.0.1",
  "thermoraw_diannv2.0.2"
)]

#4 Intersection off all sets
prots_no_diann1_8 <- Reduce(intersect, list_prots_no_diann1_8)

## Union of set of dia-nn 1.8
prots_diann1_8 <- c(
  prots_hsapiens_astral$msconvert_diannv1.8.1,
  prots_hsapiens_astral$thermoraw_diannv1.8.1
)

## Proteins identified by all versions but DIA-NN 1.8
prots_unid <- setdiff(prots_no_diann1_8, prots_diann1_8)

## All proteins identified with all versions
all_prots <- unlist(prots_hsapiens_astral) %>% 
  unique()


## Mart
mart <- useMart("ensembl", dataset = "hsapiens_gene_ensembl")


## Convert ID's
all_prots_ids <- getBM(
  attributes = c("uniprotswissprot", "entrezgene_id"),
  filters = "uniprotswissprot",
  values = all_prots,
  mart = mart
) %>% 
  pull("entrezgene_id") %>% 
  as.character()

interest_ids <- getBM(
  attributes = c("uniprotswissprot", "entrezgene_id"),
  filters = "uniprotswissprot",
  values = prots_unid,
  mart = mart
) %>% 
  pull("entrezgene_id") %>%
  as.character()


## GO Enrichment Analysis

### Biological Process
go_bp <- enrichGO(
  gene = interest_ids,
  universe = all_prots_ids,
  OrgDb = org.Hs.eg.db,
  ont = "BP",
  pAdjustMethod = "BH",
  pvalueCutoff = 0.05,
  qvalueCutoff = 0.05,
  readable = TRUE
)

### Molecular Function
go_mf <- enrichGO(
  gene = interest_ids,
  universe = all_prots_ids,
  OrgDb = org.Hs.eg.db,
  ont = "MF",
  pAdjustMethod = "BH",
  pvalueCutoff = 0.05,
  qvalueCutoff = 0.05,
  readable = TRUE
)

### Cellular Component
go_cc <- enrichGO(
  gene = interest_ids,
  universe = all_prots_ids,
  OrgDb = org.Hs.eg.db,
  ont = "CC",
  pAdjustMethod = "BH",
  pvalueCutoff = 0.05,
  qvalueCutoff = 0.05,
  readable = TRUE
)



## All ontologies
go_all <- enrichGO(
  gene = interest_ids,
  universe = all_prots_ids,
  OrgDb = org.Hs.eg.db,
  ont = "ALL",
  pAdjustMethod = "BH",
  pvalueCutoff = 0.05,
  qvalueCutoff = 0.05,
  readable = TRUE
)


## Plots
barplot(go_mf)

dotplot(go_mf)

```





# Session Info

```{r si}
sessionInfo()
```

# References


<div id="refs"></div>



